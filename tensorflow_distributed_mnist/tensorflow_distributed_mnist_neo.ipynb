{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow BYOM:\n",
    "## カスタムトレーニングスクリプトでトレーニングし、Neoでコンパイルし、SageMakerで展開\n",
    "\n",
    "このノートブックは、[TensorFlow MNIST distributed training notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_distributed_mnist/tensorflow_distributed_mnist.ipynb) の拡張版で、Amazon SageMaker Neoを用いた例です。同じ分類タスクを実行しますが、今回はNeo APIバックエンドを使用して訓練モデルをコンパイルし、ハードウェアの選択に最適化します。 最後に、Neo Deep Learning Runtimeを使用してコンパイルされたモデルのために、SageMakerにリアルタイムでホストされたエンドポイントを設定しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 環境の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST datasetのダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-3bec7422fb64>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "Writing data/train.tfrecords\n",
      "Writing data/validation.tfrecords\n",
      "Writing data/test.tfrecords\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from tensorflow.contrib.learn.python.learn.datasets import mnist\n",
    "import tensorflow as tf\n",
    "\n",
    "data_sets = mnist.read_data_sets('data', dtype=tf.uint8, reshape=False, validation_size=5000)\n",
    "\n",
    "utils.convert_to(data_sets.train, 'train', 'data')\n",
    "utils.convert_to(data_sets.validation, 'validation', 'data')\n",
    "utils.convert_to(data_sets.test, 'test', 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データのアップロード\n",
    "```sagemaker.Session.upload_data```関数を使ってデータセットをS3の場所にアップロードします。\n",
    "戻り値の`inputs`は、トレーニングジョブを開始するときに使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-314676777416\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path='data', key_prefix='data/DEMO-mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分散トレーニング用のスクリプトを作成する\n",
    "エントリポイントとして使用するネットワークモデルのコードは次のとおりです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.python.estimator.model_fn\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ModeKeys \u001b[34mas\u001b[39;49;00m Modes\r\n",
      "\r\n",
      "INPUT_TENSOR_NAME = \u001b[33m'\u001b[39;49;00m\u001b[33minputs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "SIGNATURE_NAME = \u001b[33m'\u001b[39;49;00m\u001b[33mpredictions\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "\r\n",
      "LEARNING_RATE = \u001b[34m0.001\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(features, labels, mode, params):\r\n",
      "    \u001b[37m# Input Layer\u001b[39;49;00m\r\n",
      "    input_layer = tf.reshape(features[INPUT_TENSOR_NAME], [-\u001b[34m1\u001b[39;49;00m, \u001b[34m28\u001b[39;49;00m, \u001b[34m28\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m])\r\n",
      "\r\n",
      "    \u001b[37m# Convolutional Layer #1\u001b[39;49;00m\r\n",
      "    conv1 = tf.layers.conv2d(\r\n",
      "        inputs=input_layer,\r\n",
      "        filters=\u001b[34m32\u001b[39;49;00m,\r\n",
      "        kernel_size=[\u001b[34m5\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m],\r\n",
      "        padding=\u001b[33m'\u001b[39;49;00m\u001b[33msame\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "        activation=tf.nn.relu)\r\n",
      "\r\n",
      "    \u001b[37m# Pooling Layer #1\u001b[39;49;00m\r\n",
      "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[\u001b[34m2\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m], strides=\u001b[34m2\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Convolutional Layer #2 and Pooling Layer #2\u001b[39;49;00m\r\n",
      "    conv2 = tf.layers.conv2d(\r\n",
      "        inputs=pool1,\r\n",
      "        filters=\u001b[34m64\u001b[39;49;00m,\r\n",
      "        kernel_size=[\u001b[34m5\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m],\r\n",
      "        padding=\u001b[33m'\u001b[39;49;00m\u001b[33msame\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "        activation=tf.nn.relu)\r\n",
      "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[\u001b[34m2\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m], strides=\u001b[34m2\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Dense Layer\u001b[39;49;00m\r\n",
      "    pool2_flat = tf.reshape(pool2, [-\u001b[34m1\u001b[39;49;00m, \u001b[34m7\u001b[39;49;00m * \u001b[34m7\u001b[39;49;00m * \u001b[34m64\u001b[39;49;00m])\r\n",
      "    dense = tf.layers.dense(inputs=pool2_flat, units=\u001b[34m1024\u001b[39;49;00m, activation=tf.nn.relu)\r\n",
      "    dropout = tf.layers.dropout(\r\n",
      "        inputs=dense, rate=\u001b[34m0.4\u001b[39;49;00m, training=(mode == Modes.TRAIN))\r\n",
      "\r\n",
      "    \u001b[37m# Logits Layer\u001b[39;49;00m\r\n",
      "    logits = tf.layers.dense(inputs=dropout, units=\u001b[34m10\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Define operations\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m mode \u001b[35min\u001b[39;49;00m (Modes.PREDICT, Modes.EVAL):\r\n",
      "        predicted_indices = tf.argmax(\u001b[36minput\u001b[39;49;00m=logits, axis=\u001b[34m1\u001b[39;49;00m)\r\n",
      "        probabilities = tf.nn.softmax(logits, name=\u001b[33m'\u001b[39;49;00m\u001b[33msoftmax_tensor\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m mode \u001b[35min\u001b[39;49;00m (Modes.TRAIN, Modes.EVAL):\r\n",
      "        global_step = tf.train.get_or_create_global_step()\r\n",
      "        label_indices = tf.cast(labels, tf.int32)\r\n",
      "        loss = tf.losses.softmax_cross_entropy(\r\n",
      "            onehot_labels=tf.one_hot(label_indices, depth=\u001b[34m10\u001b[39;49;00m), logits=logits)\r\n",
      "        tf.summary.scalar(\u001b[33m'\u001b[39;49;00m\u001b[33mOptimizeLoss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, loss)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m mode == Modes.PREDICT:\r\n",
      "        predictions = {\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mclasses\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: predicted_indices,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mprobabilities\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: probabilities\r\n",
      "        }\r\n",
      "        export_outputs = {\r\n",
      "            SIGNATURE_NAME: tf.estimator.export.PredictOutput(predictions)\r\n",
      "        }\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m tf.estimator.EstimatorSpec(\r\n",
      "            mode, predictions=predictions, export_outputs=export_outputs)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m mode == Modes.TRAIN:\r\n",
      "        optimizer = tf.train.AdamOptimizer(learning_rate=\u001b[34m0.001\u001b[39;49;00m)\r\n",
      "        train_op = optimizer.minimize(loss, global_step=global_step)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m mode == Modes.EVAL:\r\n",
      "        eval_metric_ops = {\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: tf.metrics.accuracy(label_indices, predicted_indices)\r\n",
      "        }\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m tf.estimator.EstimatorSpec(\r\n",
      "            mode, loss=loss, eval_metric_ops=eval_metric_ops)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mserving_input_fn\u001b[39;49;00m(params):\r\n",
      "    inputs = {INPUT_TENSOR_NAME: tf.placeholder(tf.float32, [\u001b[36mNone\u001b[39;49;00m, \u001b[34m784\u001b[39;49;00m])}\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m tf.estimator.export.ServingInputReceiver(inputs, inputs)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mread_and_decode\u001b[39;49;00m(filename_queue):\r\n",
      "    reader = tf.TFRecordReader()\r\n",
      "    _, serialized_example = reader.read(filename_queue)\r\n",
      "\r\n",
      "    features = tf.parse_single_example(\r\n",
      "        serialized_example,\r\n",
      "        features={\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mimage_raw\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: tf.FixedLenFeature([], tf.string),\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: tf.FixedLenFeature([], tf.int64),\r\n",
      "        })\r\n",
      "\r\n",
      "    image = tf.decode_raw(features[\u001b[33m'\u001b[39;49;00m\u001b[33mimage_raw\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], tf.uint8)\r\n",
      "    image.set_shape([\u001b[34m784\u001b[39;49;00m])\r\n",
      "    image = tf.cast(image, tf.float32) * (\u001b[34m1.\u001b[39;49;00m / \u001b[34m255\u001b[39;49;00m)\r\n",
      "    label = tf.cast(features[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], tf.int32)\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m image, label\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain_input_fn\u001b[39;49;00m(training_dir, params):\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m _input_fn(training_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain.tfrecords\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, batch_size=\u001b[34m100\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32meval_input_fn\u001b[39;49;00m(training_dir, params):\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m _input_fn(training_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtest.tfrecords\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, batch_size=\u001b[34m100\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_input_fn\u001b[39;49;00m(training_dir, training_filename, batch_size=\u001b[34m100\u001b[39;49;00m):\r\n",
      "    test_file = os.path.join(training_dir, training_filename)\r\n",
      "    filename_queue = tf.train.string_input_producer([test_file])\r\n",
      "\r\n",
      "    image, label = read_and_decode(filename_queue)\r\n",
      "    images, labels = tf.train.batch(\r\n",
      "        [image, label], batch_size=batch_size,\r\n",
      "        capacity=\u001b[34m1000\u001b[39;49;00m + \u001b[34m3\u001b[39;49;00m * batch_size)\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m {INPUT_TENSOR_NAME: images}, labels\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mneo_preprocess\u001b[39;49;00m(payload, content_type):\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mPIL.Image\u001b[39;49;00m   \u001b[37m# Training container doesn't have this package\u001b[39;49;00m\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mio\u001b[39;49;00m\r\n",
      "\r\n",
      "    logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33mInvoking user-defined pre-processing function\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m content_type != \u001b[33m'\u001b[39;49;00m\u001b[33mapplication/x-image\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mContent type must be application/x-image\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    f = io.BytesIO(payload)\r\n",
      "    \u001b[37m# Load image and convert to greyscale space\u001b[39;49;00m\r\n",
      "    image = PIL.Image.open(f).convert(\u001b[33m'\u001b[39;49;00m\u001b[33mL\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[37m# Resize\u001b[39;49;00m\r\n",
      "    image = np.asarray(image.resize((\u001b[34m28\u001b[39;49;00m, \u001b[34m28\u001b[39;49;00m)))\r\n",
      "    \u001b[37m# Reshape\u001b[39;49;00m\r\n",
      "    image = image.reshape((\u001b[34m1\u001b[39;49;00m,-\u001b[34m1\u001b[39;49;00m)).astype(\u001b[33m'\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m image\r\n",
      "\r\n",
      "\u001b[37m### NOTE: this function cannot use MXNet\u001b[39;49;00m\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mneo_postprocess\u001b[39;49;00m(result):\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\r\n",
      "    logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33mInvoking user-defined post-processing function\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[37m# Softmax (assumes batch size 1)\u001b[39;49;00m\r\n",
      "    result = np.squeeze(result)\r\n",
      "    result_exp = np.exp(result - np.max(result))\r\n",
      "    result = result_exp / np.sum(result_exp)\r\n",
      "\r\n",
      "    response_body = json.dumps(result.tolist())\r\n",
      "    content_type = \u001b[33m'\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m response_body, content_type\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize 'mnist.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このスクリプトは、[TensorFlow MNIST example](https://github.com/tensorflow/models/tree/master/official/mnist)です。 \n",
    "`` model_fn（機能、ラベル、モード） ``を提供します。これは、学習、評価、推論に使用されます。 トレーニングスクリプトの詳細については、[TensorFlow MNIST distributed training notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_distributed_mnist/tensorflow_distributed_mnist.ipynb) を参照してください。\n",
    "\n",
    "トレーニングスクリプトの最後には、Neo Deep Learning Runtimeで使用する2つの追加機能があります。\n",
    "* `neo_preprocess（payload、content_type）`：各受信リクエストのペイロードとContent-Typeを受け取り、NumPy配列を返す関数\n",
    "* `neo_postprocess（result）`：Deep Learining Runtimeによって生成された予測結果を受け取り、応答本体を返す関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sagemaker.TensorFlow estimatorを使用してトレーニングジョブを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-314676777416\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-tensorflow-2018-12-17-17-52-05-942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-17 17:52:06 Starting - Starting the training job...\n",
      "2018-12-17 17:52:07 Starting - Launching requested ML instances......\n",
      "2018-12-17 17:53:09 Starting - Preparing the instances for training.....\n",
      "\u001b[31m2018-12-17 17:54:19,112 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:19,112 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:19,125 INFO - container_support.training - Training starting\u001b[0m\n",
      "\u001b[31mDownloading s3://sagemaker-us-east-1-314676777416/sagemaker-tensorflow-2018-12-17-17-52-05-942/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:21,078 INFO - tf_container - ----------------------TF_CONFIG--------------------------\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:21,078 INFO - tf_container - {\"environment\": \"cloud\", \"cluster\": {\"worker\": [\"algo-2:2222\"], \"ps\": [\"algo-1:2223\", \"algo-2:2223\"], \"master\": [\"algo-1:2222\"]}, \"task\": {\"index\": 0, \"type\": \"master\"}}\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:21,078 INFO - tf_container - ---------------------------------------------------------\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:21,078 INFO - tf_container - creating RunConfig:\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:21,078 INFO - tf_container - {'save_checkpoints_secs': 300}\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:21,078 INFO - tensorflow - TF_CONFIG environment variable: {u'environment': u'cloud', u'cluster': {u'ps': [u'algo-1:2223', u'algo-2:2223'], u'worker': [u'algo-2:2222'], u'master': [u'algo-1:2222']}, u'task': {u'index': 0, u'type': u'master'}}\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:21,079 INFO - tf_container - creating an estimator from the user-provided model_fn\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:21,079 INFO - tensorflow - Using config: {'_save_checkpoints_secs': 300, '_keep_checkpoint_max': 5, '_task_type': u'master', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4fa6789350>, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 2, '_tf_random_seed': None, '_device_fn': None, '_num_worker_replicas': 2, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_session_config': device_filters: \"/job:ps\"\u001b[0m\n",
      "\u001b[31mdevice_filters: \"/job:master\"\u001b[0m\n",
      "\u001b[31mallow_soft_placement: true\u001b[0m\n",
      "\u001b[31mgraph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\u001b[0m\n",
      "\u001b[31m}\u001b[0m\n",
      "\u001b[31m, '_global_id_in_cluster': 0, '_is_chief': True, '_protocol': None, '_save_checkpoints_steps': None, '_experimental_distribute': None, '_save_summary_steps': 100, '_model_dir': u's3://sagemaker-us-east-1-314676777416/sagemaker-tensorflow-2018-12-17-17-52-05-942/checkpoints', '_master': u'grpc://algo-1:2222'}\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:21,080 INFO - tensorflow - Start Tensorflow server.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:21.174819: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:21.175304: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:21,341 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:187: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:21,342 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:187: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:21,361 INFO - tensorflow - Calling model_fn.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:21,614 INFO - tensorflow - Done calling model_fn.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:21,615 INFO - tensorflow - Create CheckpointSaverHook.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:21.622922: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:21.622952: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:21.640074: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:21.640102: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:21.659886: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:21.659916: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:21,961 INFO - tensorflow - Graph was finalized.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:21.967345: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:21.967379: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:24,418 INFO - tensorflow - Running local_init_op.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:24,423 INFO - tensorflow - Done running local_init_op.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:24,451 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:24.512912: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:24.512948: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\n",
      "2018-12-17 17:54:21 Downloading - Downloading input data\n",
      "2018-12-17 17:54:21 Training - Training image download completed. Training in progress.\u001b[32m2018-12-17 17:54:21,191 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[32m2018-12-17 17:54:21,191 INFO - root - starting train task\u001b[0m\n",
      "\u001b[32m2018-12-17 17:54:21,203 INFO - container_support.training - Training starting\u001b[0m\n",
      "\u001b[32mDownloading s3://sagemaker-us-east-1-314676777416/sagemaker-tensorflow-2018-12-17-17-52-05-942/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[32m2018-12-17 17:54:23,185 INFO - tf_container - ----------------------TF_CONFIG--------------------------\u001b[0m\n",
      "\u001b[32m2018-12-17 17:54:23,185 INFO - tf_container - {\"environment\": \"cloud\", \"cluster\": {\"worker\": [\"algo-2:2222\"], \"ps\": [\"algo-1:2223\", \"algo-2:2223\"], \"master\": [\"algo-1:2222\"]}, \"task\": {\"index\": 0, \"type\": \"worker\"}}\u001b[0m\n",
      "\u001b[32m2018-12-17 17:54:23,185 INFO - tf_container - ---------------------------------------------------------\u001b[0m\n",
      "\u001b[32m2018-12-17 17:54:23,185 INFO - tf_container - creating RunConfig:\u001b[0m\n",
      "\u001b[32m2018-12-17 17:54:23,185 INFO - tf_container - {'save_checkpoints_secs': 300}\u001b[0m\n",
      "\u001b[32m2018-12-17 17:54:23,186 INFO - tensorflow - TF_CONFIG environment variable: {u'environment': u'cloud', u'cluster': {u'ps': [u'algo-1:2223', u'algo-2:2223'], u'worker': [u'algo-2:2222'], u'master': [u'algo-1:2222']}, u'task': {u'index': 0, u'type': u'worker'}}\u001b[0m\n",
      "\u001b[32m2018-12-17 17:54:23,186 INFO - tf_container - creating an estimator from the user-provided model_fn\u001b[0m\n",
      "\u001b[32m2018-12-17 17:54:23,187 INFO - tensorflow - Using config: {'_save_checkpoints_secs': 300, '_keep_checkpoint_max': 5, '_task_type': u'worker', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd0f6bca8d0>, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 2, '_tf_random_seed': None, '_device_fn': None, '_num_worker_replicas': 2, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_session_config': device_filters: \"/job:ps\"\u001b[0m\n",
      "\u001b[32mdevice_filters: \"/job:worker/task:0\"\u001b[0m\n",
      "\u001b[32mallow_soft_placement: true\u001b[0m\n",
      "\u001b[32mgraph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\u001b[0m\n",
      "\u001b[32m}\u001b[0m\n",
      "\u001b[32m, '_global_id_in_cluster': 1, '_is_chief': False, '_protocol': None, '_save_checkpoints_steps': None, '_experimental_distribute': None, '_save_summary_steps': 100, '_model_dir': u's3://sagemaker-us-east-1-314676777416/sagemaker-tensorflow-2018-12-17-17-52-05-942/checkpoints', '_master': u'grpc://algo-2:2222'}\u001b[0m\n",
      "\u001b[32m2018-12-17 17:54:23,188 INFO - tensorflow - Start Tensorflow server.\u001b[0m\n",
      "\u001b[32m2018-12-17 17:54:23,201 INFO - tensorflow - Waiting 5 secs before starting training.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:24,933 INFO - tensorflow - Saving checkpoints for 0 into s3://sagemaker-us-east-1-314676777416/sagemaker-tensorflow-2018-12-17-17-52-05-942/checkpoints/model.ckpt.\u001b[0m\n",
      "\u001b[32m2018-12-17 17:54:28.255799: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[32m2018-12-17 17:54:28.256253: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[32m2018-12-17 17:54:28,403 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:187: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[32m2018-12-17 17:54:28,405 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:187: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[32m2018-12-17 17:54:28,423 INFO - tensorflow - Calling model_fn.\u001b[0m\n",
      "\u001b[32m2018-12-17 17:54:28,662 INFO - tensorflow - Done calling model_fn.\u001b[0m\n",
      "\u001b[32m2018-12-17 17:54:28,663 INFO - tensorflow - Create CheckpointSaverHook.\u001b[0m\n",
      "\u001b[32m2018-12-17 17:54:28,763 INFO - tensorflow - Graph was finalized.\u001b[0m\n",
      "\u001b[32m2018-12-17 17:54:28,817 INFO - tensorflow - Running local_init_op.\u001b[0m\n",
      "\u001b[32m2018-12-17 17:54:28,823 INFO - tensorflow - Done running local_init_op.\u001b[0m\n",
      "\u001b[32m2018-12-17 17:54:28,852 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[32mInstructions for updating:\u001b[0m\n",
      "\u001b[32mTo construct input pipelines, use the `tf.data` module.\u001b[0m\n",
      "\u001b[32m2018-12-17 17:54:29,233 INFO - tensorflow - loss = 2.3162155, step = 0\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:30.605649: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:30.605684: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:31,297 INFO - tensorflow - loss = 0.37889674, step = 19\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m2018-12-17 17:54:38,084 INFO - tensorflow - global_step/sec: 15.3203\u001b[0m\n",
      "\u001b[32m2018-12-17 17:54:41,695 INFO - tensorflow - loss = 0.042283025, step = 179 (12.463 sec)\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:44,422 INFO - tensorflow - loss = 0.029699039, step = 220 (13.125 sec)\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:44,804 INFO - tensorflow - global_step/sec: 15.1804\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:51,253 INFO - tensorflow - global_step/sec: 15.6614\u001b[0m\n",
      "\u001b[32m2018-12-17 17:54:55,239 INFO - tensorflow - loss = 0.0490227, step = 387 (13.544 sec)\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:56,679 INFO - tensorflow - loss = 0.045940056, step = 411 (12.256 sec)\u001b[0m\n",
      "\u001b[31m2018-12-17 17:54:57,825 INFO - tensorflow - global_step/sec: 15.52\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:04,156 INFO - tensorflow - global_step/sec: 15.7951\u001b[0m\n",
      "\u001b[32m2018-12-17 17:55:08,787 INFO - tensorflow - loss = 0.056856263, step = 600 (13.548 sec)\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:08,691 INFO - tensorflow - loss = 0.06102873, step = 599 (12.012 sec)\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:10,656 INFO - tensorflow - global_step/sec: 15.5388\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:17,076 INFO - tensorflow - global_step/sec: 15.7323\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:21,159 INFO - tensorflow - loss = 0.076003864, step = 794 (12.468 sec)\u001b[0m\n",
      "\u001b[32m2018-12-17 17:55:21,928 INFO - tensorflow - loss = 0.010046069, step = 806 (13.140 sec)\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:23,532 INFO - tensorflow - global_step/sec: 15.6425\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:30,171 INFO - tensorflow - global_step/sec: 15.3635\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:33,355 INFO - tensorflow - loss = 0.002944536, step = 983 (12.196 sec)\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:34,318 INFO - tensorflow - Saving checkpoints for 1001 into s3://sagemaker-us-east-1-314676777416/sagemaker-tensorflow-2018-12-17-17-52-05-942/checkpoints/model.ckpt.\u001b[0m\n",
      "\u001b[32m2018-12-17 17:55:34,456 INFO - tensorflow - Loss for final step: 0.0028163316.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:36.987981: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:36.988022: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:37,350 INFO - tensorflow - Calling model_fn.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:37,446 INFO - tensorflow - Done calling model_fn.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:37,461 INFO - tensorflow - Starting evaluation at 2018-12-17-17:55:37\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:37,585 INFO - tensorflow - Graph was finalized.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:37,681 INFO - tensorflow - Restoring parameters from s3://sagemaker-us-east-1-314676777416/sagemaker-tensorflow-2018-12-17-17-52-05-942/checkpoints/model.ckpt-1001\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:38,037 INFO - tensorflow - Running local_init_op.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:38,047 INFO - tensorflow - Done running local_init_op.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:38,376 INFO - tensorflow - Evaluation [10/100]\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:38,598 INFO - tensorflow - Evaluation [20/100]\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:38,805 INFO - tensorflow - Evaluation [30/100]\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:39,013 INFO - tensorflow - Evaluation [40/100]\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:39,224 INFO - tensorflow - Evaluation [50/100]\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:39,452 INFO - tensorflow - Evaluation [60/100]\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:39,649 INFO - tensorflow - Evaluation [70/100]\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:39,874 INFO - tensorflow - Evaluation [80/100]\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:40,077 INFO - tensorflow - Evaluation [90/100]\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:40,306 INFO - tensorflow - Evaluation [100/100]\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:40,336 INFO - tensorflow - Finished evaluation at 2018-12-17-17:55:40\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:40,336 INFO - tensorflow - Saving dict for global step 1001: accuracy = 0.9858, global_step = 1001, loss = 0.03986918\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:40.345044: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:40.345077: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:40.365207: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:40.365243: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:40.382110: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:40.382142: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:40,768 INFO - tensorflow - Saving 'checkpoint_path' summary for global step 1001: s3://sagemaker-us-east-1-314676777416/sagemaker-tensorflow-2018-12-17-17-52-05-942/checkpoints/model.ckpt-1001\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:40.883406: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:40.883442: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:40.902502: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:40.902534: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:40.920394: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:40.920425: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:40.940510: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:40.940540: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:40.964989: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:40.965021: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:40.984836: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:40.984865: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:41,067 INFO - tensorflow - Calling model_fn.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:41,115 INFO - tensorflow - Done calling model_fn.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:41,116 INFO - tensorflow - Signatures INCLUDED in export for Eval: None\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:41,116 INFO - tensorflow - Signatures INCLUDED in export for Classify: None\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:41,116 INFO - tensorflow - Signatures INCLUDED in export for Regress: None\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:41,116 INFO - tensorflow - Signatures INCLUDED in export for Predict: ['serving_default', 'predictions']\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:41,116 INFO - tensorflow - Signatures INCLUDED in export for Train: None\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:41,189 INFO - tensorflow - Restoring parameters from s3://sagemaker-us-east-1-314676777416/sagemaker-tensorflow-2018-12-17-17-52-05-942/checkpoints/model.ckpt-1001\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:41,659 WARNING - tensorflow - From /usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py:1018: calling add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mPass your op to the equivalent parameter main_op instead.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:41,659 INFO - tensorflow - Assets added to graph.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:41,659 INFO - tensorflow - No assets to write.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:41.677923: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:41.677961: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:41.694529: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:41.694561: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:41.714383: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:41.714416: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:43.246717: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:43.246761: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:43,301 INFO - tensorflow - SavedModel written to: s3://sagemaker-us-east-1-314676777416/sagemaker-tensorflow-2018-12-17-17-52-05-942/checkpoints/export/Servo/temp-1545069340/saved_model.pb\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:43.307640: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:43.307676: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:44.022558: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:44.022605: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:44.046396: E tensorflow/core/platform/s3/aws_logging.cc:60] No response body. Response code: 404\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:44.046433: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:44,184 INFO - tensorflow - Loss for final step: 0.040506475.\u001b[0m\n",
      "\u001b[31m2018-12-17 17:55:44,480 INFO - tf_container - Downloaded saved model at /opt/ml/model/export/Servo/1545069340\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2018-12-17 17:55:55 Uploading - Uploading generated training model\n",
      "2018-12-17 17:55:55 Completed - Training job completed\n",
      "Billable seconds: 211\n",
      "CPU times: user 425 ms, sys: 52.8 ms, total: 478 ms\n",
      "Wall time: 4min 13s\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "mnist_estimator = TensorFlow(entry_point='mnist.py',\n",
    "                             role=role,\n",
    "                             framework_version='1.11.0',\n",
    "                             training_steps=1000, \n",
    "                             evaluation_steps=100,\n",
    "                             train_instance_count=2,\n",
    "                             train_instance_type='ml.c5.2xlarge')\n",
    "\n",
    "%time mnist_estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **```fit```** method will create a training job in two **ml.c5.2xlarge** instances. The logs above will show the instances doing training, evaluation, and incrementing the number of **training steps**. \n",
    "\n",
    "In the end of the training, the training job will generate a saved model for TF serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**```fit```** メソッドは、2つの **ml.c5.2xlarge** インスタンスでトレーニングジョブを作成します。 上記のログには、トレーニング、評価、**トレーニングステップ数の増分** などのインスタンスが表示されます。\n",
    "\n",
    "訓練の最後に、訓練の仕事は、TF提供のために保存されたモデルを生成するでしょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the trained model to prepare for predictions (the old way)\n",
    "\n",
    "The deploy() method creates an endpoint which serves prediction requests in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練されたモデルを展開して予測を準備する（既存の手法）\n",
    "\n",
    "deploy（）メソッドは、予測リクエストをリアルタイムで処理するエンドポイントを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_predictor = mnist_estimator.deploy(initial_instance_count=1,\n",
    "                                         instance_type='ml.c5.4xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エンドポイントの呼び出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "for i in range(10):\n",
    "    data = mnist.test.images[i].tolist()\n",
    "    tensor_proto = tf.make_tensor_proto(values=np.asarray(data), shape=[1, len(data)], dtype=tf.float32)\n",
    "    predict_response = mnist_predictor.predict(tensor_proto)\n",
    "    \n",
    "    print(\"========================================\")\n",
    "    label = np.argmax(mnist.test.labels[i])\n",
    "    print(\"label is {}\".format(label))\n",
    "    prediction = predict_response['outputs']['classes']['int64_val'][0]\n",
    "    print(\"prediction is {}\".format(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エンドポイントの削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(mnist_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the trained model using Neo\n",
    "\n",
    "Now the model is ready to be compiled by Neo to be optimized for our hardware of choice. We are using the  ``TensorFlowEstimator.compile_model`` method to do this. For this example, our target hardware is ``'ml_c5'``. You can changed these to other supported target hardware if you prefer.\n",
    "\n",
    "## Compiling the model\n",
    "The ``input_shape`` is the definition for the model's input tensor and ``output_path`` is where the compiled model will be stored in S3. **Important. If the following command result in a permission error, scroll up and locate the value of execution role returned by `get_execution_role()`. The role must have access to the S3 bucket specified in ``output_path``.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neoを使用して訓練されたモデルを展開する\n",
    "\n",
    "これでモデルはNeoによってコンパイルされ、選択したハードウェアに最適化されました。 これを行うには `` TensorFlowEstimator.compile_model``メソッドを使用しています。 この例では、ターゲットハードウェアは `` 'ml_c5'``です。 必要に応じて、これらを他のサポートされているターゲットハードウェアに変更できます。\n",
    "\n",
    "### モデルをコンパイルする\n",
    "`` input_shape``はモデルの入力テンソルの定義で、 `` output_path``はコンパイルされたモデルがS3に格納される場所です。 **重要。 次のコマンドでパーミッションエラーが発生した場合は、スクロールして `get_execution_role（）`によって返された実行ロールの値を探します。 ロールは `` output_path``で指定されたS3バケットにアクセスする必要があります。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/'.join(mnist_estimator.output_path.split('/')[:-1])\n",
    "optimized_estimator = mnist_estimator.compile_model(target_instance_family='ml_c5', \n",
    "                              input_shape={'data':[1, 784]},  # Batch size 1, 3 channels, 224x224 Images.\n",
    "                              output_path=output_path,\n",
    "                              framework='tensorflow', framework_version='1.11.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## コンパイルされたモデルのデプロイ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_predictor = optimized_estimator.deploy(initial_instance_count = 1,\n",
    "                                                 instance_type = 'ml.c5.4xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The neo_preprocess() function expects an image in the request body\n",
    "# But the MNIST example data is saved as NumPy array.\n",
    "# So we convert it to PNG before invoking the endpoint\n",
    "def png_serializer(data):\n",
    "    im = PIL.Image.fromarray(data.reshape((28,28))*255).convert('L')\n",
    "    f = io.BytesIO()\n",
    "    im.save(f, format='png')\n",
    "    f.seek(0)\n",
    "    return f.read()\n",
    "\n",
    "optimized_predictor.content_type = 'application/x-image'\n",
    "optimized_predictor.serializer = png_serializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エンドポイントの読み出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from IPython import display\n",
    "import PIL.Image\n",
    "import io\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "for i in range(10):\n",
    "    data = mnist.test.images[i]\n",
    "    # Display image\n",
    "    im = PIL.Image.fromarray(data.reshape((28,28))*255).convert('L')\n",
    "    display.display(im)\n",
    "    # Invoke endpoint with image\n",
    "    predict_response = optimized_predictor.predict(data)\n",
    "    \n",
    "    print(\"========================================\")\n",
    "    label = np.argmax(mnist.test.labels[i])\n",
    "    print(\"label is {}\".format(label))\n",
    "    prediction = predict_response\n",
    "    print(\"prediction is {}\".format(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エンドポイントの削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(optimized_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
